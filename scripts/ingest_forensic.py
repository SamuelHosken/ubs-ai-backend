#!/usr/bin/env python3
"""
Script de ingestÃ£o completa de todos os dados forenses.
Processa statements, fees, timeline, documentos oficiais UBS e anÃ¡lises forenses.
"""
import sys
import os
from pathlib import Path

# Adicionar o diretÃ³rio raiz ao path
sys.path.insert(0, str(Path(__file__).parent.parent))

from dotenv import load_dotenv
load_dotenv()

from app.services.embedding_service import EmbeddingService
from app.models.chunks import ChunkCategory
from app.processors import (
    StatementsProcessor,
    FeesProcessor,
    TimelineProcessor,
    ForensicProcessor,
    UBSDocsProcessor
)


def print_header(text: str):
    """Imprime header formatado"""
    print("\n" + "=" * 60)
    print(f"  {text}")
    print("=" * 60)


def print_stats(stats: dict):
    """Imprime estatÃ­sticas"""
    print("\n" + "-" * 40)
    print("ESTATÃSTICAS:")
    print("-" * 40)
    for name, count in stats.items():
        print(f"  {name}: {count} chunks")
    print("-" * 40)
    print(f"  TOTAL: {sum(stats.values())} chunks")


def ingest_statements(embedding_service: EmbeddingService, base_path: Path) -> int:
    """Ingere statements"""
    print_header("ðŸ“Š Processando Statements...")

    processor = StatementsProcessor(str(base_path / "statements"))
    chunks = processor.process_all()

    if chunks:
        chunk_dicts = [
            {
                "chunk_id": c.chunk_id,
                "content": c.content,
                "metadata": c.model_dump(exclude={"content", "chunk_id", "content_pt"})
            }
            for c in chunks
        ]

        embedding_service.add_chunks_batch(ChunkCategory.FACTS, chunk_dicts)

    print(f"\nâœ… {len(chunks)} chunks de statements adicionados")
    return len(chunks)


def ingest_fees(embedding_service: EmbeddingService, base_path: Path) -> int:
    """Ingere fees"""
    print_header("ðŸ’° Processando Fees...")

    processor = FeesProcessor(str(base_path / "fees"))
    chunks = processor.process_all()

    if chunks:
        chunk_dicts = [
            {
                "chunk_id": c.chunk_id,
                "content": c.content,
                "metadata": c.model_dump(exclude={"content", "chunk_id", "content_pt"})
            }
            for c in chunks
        ]

        embedding_service.add_chunks_batch(ChunkCategory.FACTS, chunk_dicts)

    print(f"\nâœ… {len(chunks)} chunks de fees adicionados")
    return len(chunks)


def ingest_timeline(embedding_service: EmbeddingService, base_path: Path) -> dict:
    """Ingere timeline"""
    print_header("ðŸ“… Processando Timeline...")

    processor = TimelineProcessor(str(base_path / "timeline"))
    results = processor.process_all()

    stats = {"context": 0, "client": 0}

    # Chunks de contexto histÃ³rico
    context_chunks = results.get("context", [])
    if context_chunks:
        chunk_dicts = [
            {
                "chunk_id": c.chunk_id,
                "content": c.content,
                "metadata": c.model_dump(exclude={"content", "chunk_id", "content_pt"})
            }
            for c in context_chunks
        ]
        embedding_service.add_chunks_batch(ChunkCategory.CONTEXT, chunk_dicts)
        stats["context"] = len(context_chunks)
        print(f"  âœ… {len(context_chunks)} chunks de contexto histÃ³rico")

    # Chunks do cliente
    client_chunks = results.get("client", [])
    if client_chunks:
        chunk_dicts = [
            {
                "chunk_id": c.chunk_id,
                "content": c.content,
                "metadata": c.model_dump(exclude={"content", "chunk_id", "content_pt"})
            }
            for c in client_chunks
        ]
        embedding_service.add_chunks_batch(ChunkCategory.CLIENT, chunk_dicts)
        stats["client"] = len(client_chunks)
        print(f"  âœ… {len(client_chunks)} chunks de timeline do cliente")

    return stats


def ingest_forensic(embedding_service: EmbeddingService, base_path: Path) -> int:
    """Ingere anÃ¡lises forenses"""
    print_header("ðŸ” Processando AnÃ¡lises Forenses...")

    processor = ForensicProcessor(str(base_path / "forensic"))
    chunks = processor.process_all()

    if chunks:
        chunk_dicts = [
            {
                "chunk_id": c.chunk_id,
                "content": c.content,
                "metadata": c.model_dump(exclude={"content", "chunk_id", "content_pt"})
            }
            for c in chunks
        ]

        embedding_service.add_chunks_batch(ChunkCategory.FORENSIC, chunk_dicts)

    print(f"\nâœ… {len(chunks)} chunks forenses adicionados")
    return len(chunks)


def ingest_ubs_official(embedding_service: EmbeddingService, base_path: Path) -> int:
    """Ingere documentos oficiais da UBS"""
    print_header("ðŸ“œ Processando Documentos Oficiais UBS...")

    processor = UBSDocsProcessor(str(base_path / "ubs_official"))
    chunks = processor.process_all()

    if chunks:
        chunk_dicts = [
            {
                "chunk_id": c.chunk_id,
                "content": c.content,
                "metadata": c.model_dump(exclude={"content", "chunk_id", "content_pt"})
            }
            for c in chunks
        ]

        embedding_service.add_chunks_batch(ChunkCategory.UBS_OFFICIAL, chunk_dicts)

    print(f"\nâœ… {len(chunks)} chunks de docs oficiais UBS adicionados")
    return len(chunks)


def main():
    print("\n")
    print("â•”" + "â•" * 58 + "â•—")
    print("â•‘" + " " * 10 + "ðŸš€ INGESTÃƒO FORENSE - RAG UBS" + " " * 18 + "â•‘")
    print("â•š" + "â•" * 58 + "â•")

    # Caminho base dos dados
    base_path = Path(__file__).parent.parent / "data" / "raw"

    if not base_path.exists():
        print(f"\nâŒ Pasta de dados nÃ£o encontrada: {base_path}")
        print("Crie a pasta e adicione os documentos.")
        return

    # Inicializar serviÃ§o de embeddings
    print("\nðŸ”§ Inicializando serviÃ§o de embeddings...")
    embedding_service = EmbeddingService()

    # Limpar collections antes de popular (evita duplicatas)
    print("\nðŸ§¹ Limpando collections existentes...")
    for category in ChunkCategory:
        try:
            embedding_service.clear_collection(category)
            print(f"  âœ“ {category.value} limpa")
        except Exception as e:
            print(f"  âš ï¸ {category.value}: {e}")

    # EstatÃ­sticas
    stats = {}

    # 1. Statements
    if (base_path / "statements").exists():
        stats["statements"] = ingest_statements(embedding_service, base_path)
    else:
        print("\nâš ï¸  Pasta statements/ nÃ£o encontrada, pulando...")

    # 2. Fees
    if (base_path / "fees").exists():
        stats["fees"] = ingest_fees(embedding_service, base_path)
    else:
        print("\nâš ï¸  Pasta fees/ nÃ£o encontrada, pulando...")

    # 3. Timeline
    if (base_path / "timeline").exists():
        timeline_stats = ingest_timeline(embedding_service, base_path)
        stats["context"] = timeline_stats.get("context", 0)
        stats["client"] = timeline_stats.get("client", 0)
    else:
        print("\nâš ï¸  Pasta timeline/ nÃ£o encontrada, pulando...")

    # 4. Forensic
    if (base_path / "forensic").exists():
        stats["forensic"] = ingest_forensic(embedding_service, base_path)
    else:
        print("\nâš ï¸  Pasta forensic/ nÃ£o encontrada, pulando...")

    # 5. Documentos oficiais UBS
    if (base_path / "ubs_official").exists():
        stats["ubs_official"] = ingest_ubs_official(embedding_service, base_path)
    else:
        print("\nâš ï¸  Pasta ubs_official/ nÃ£o encontrada, pulando...")

    # Resumo final
    print_header("âœ… INGESTÃƒO COMPLETA!")
    print_stats(stats)

    # EstatÃ­sticas do ChromaDB
    print("\nðŸ“Š EstatÃ­sticas do ChromaDB:")
    chroma_stats = embedding_service.get_all_collection_stats()
    for collection, count in chroma_stats.items():
        if count > 0:
            print(f"  {collection}: {count} documentos")

    print("\n" + "=" * 60)
    print("  ðŸŽ‰ Pronto para usar o RAG Forense!")
    print("=" * 60 + "\n")


if __name__ == "__main__":
    main()
